# Paper List

## Oct. 31, 2025

### Efficiency in Large Language Models
Presenter: Wenjie

#### KV Cache Compression
|paper|conference|date|institution|group|
|---|---|---|---|---|
|[H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://arxiv.org/abs/2306.14048)|NeurIPS'23|June 23|CMU|Beidi Chen|
|[SnapKV: LLM Knows What You Are Looking for before Generation](https://arxiv.org/abs/2404.14469)|NeurIPS'24|Apr 24|UIUC| Deming Chen|
|[R-KV: Redundancy-aware KV Cache Compression for Reasoning Models](https://arxiv.org/abs/2505.24133)|NeurIPS'25|May 25|UWisconsin|Junjie Hu|
|[PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference](https://arxiv.org/abs/2405.12532)|ACL'24 Findings|May 24|SJTU|Hai Zhao|
|[CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences](https://arxiv.org/abs/2503.12491)|ICLR'25|Mar 25|Ant|Jianguo Li|
|[Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs](https://arxiv.org/abs/2310.01801)|ICLR'24|Oct 23|MSR|Jianfeng Gao|
|[Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning](https://arxiv.org/abs/2410.19258)|ICLR'25|Oct 24|MSR|Wen Xiao|
|[DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](https://arxiv.org/abs/2410.10819)|ICLR'25|Oct 24|MIT|Song Han|
|[RazorAttention: Efficient KV Cache Compression Through Retrieval Heads](https://arxiv.org/abs/2407.15891)|ICLR'25|July 24|Huawei|Gongyi Wang|
|[Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?](https://arxiv.org/abs/2506.17121)|arXiv|June 25|Princeton|Danqi Chen|
|[Which Heads Matter for Reasoning? RL-Guided KV Cache Compression](https://arxiv.org/abs/2510.08525)|arXiv|Oct 25|Westlake|Huan Wang|

#### Sparse Attention
|paper|conference|date|institution|group|
|---|---|---|---|---|
|[Efficient streaming language models with attention sinks](https://arxiv.org/abs/2309.17453)|ICLR’24|Sept 23|MIT|Song Han|
|[MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](https://arxiv.org/abs/2407.02490)|NeurIPS’24|July 24|MSR|Lili Qiu|
|[Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2406.10774)|ICML’24|June 24|MIT|Song Han|
|[Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2502.11089)|ACL'25 (best paper award)|Feb 25|DeepSeek-AI|DeepSeek-AI|

#### Efficiency for Training
|paper|conference|date|institution|group|
|---|---|---|---|---|
|[Your Efficient RL Framework Secretly Brings You Off-Policy RL Training](https://fengyao.notion.site/off-policy-rl#279721e3f6c48032af82d31498e49c5d)|blog|Aug 25|MS|Jianfeng Gao|
|[On-Policy Distillation](https://thinkingmachines.ai/blog/on-policy-distillation/)|blog|Oct 25|Thinking Machines Lab|Thinking Machines Lab|
